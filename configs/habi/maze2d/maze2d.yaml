defaults:
  - _self_
  - task: maze2d-umaze-v1
  - reward_mode: linear

pipeline_name: habi_d4rl_maze2d
mode: train
seed: 0
device: cuda:0
project: Habi
group: release
name: Default
enable_wandb: 1
save_dir: results

# Guidance
guidance_type: MCSS # cfg, cg, MCSS
# For Planner
planner_net: transformer # transformer, unet
# Separate state and action
pipeline_type: separate # agent, planneronly
# Rebase_policy position
rebase_policy: true

latent_dim: 256
deterministic_latent: 0
planner_input_type: state
reconstruct_eta: 0
learn_kl_eta: 1
kl_eta: 1.0
dql_use_diffusion: 1

# Environment

# Planner Config
planner_solver: ddim
## Network Architecture
planner_emb_dim: 128
planner_d_model: 256
planner_depth: 2
planner_sampling_steps: 20
planner_predict_noise: True
planner_next_obs_loss_weight: 1
planner_ema_rate: 0.9999
unet_dim: 32
critic_learning_rate: 0.0003

## Policy Config
policy_solver: ddpm
policy_hidden_dim: 256
policy_diffusion_steps: 10
policy_sampling_steps: 10
policy_predict_noise: True
policy_ema_rate: 0.995
policy_learning_rate: 0.0003

## DQL Policy Config
dql_solver: ddpm
dql_hidden_dim: 256
dql_diffusion_steps: 10
dql_sampling_steps: 10
dql_predict_noise: True
dql_ema_rate: 0.995
dql_learning_rate: 0.0003
dql_ema_update_interval: 5

# Training
use_diffusion_invdyn: 1
# invdyn_gradient_steps: 200000
# policy_diffusion_gradient_steps: 1000000
# planner_diffusion_gradient_steps: 1000000
gradient_steps: 1000000
bb_ckpt: 1000000
batch_size: 256
log_interval: 4000
save_interval: 40000
uncondition_sample_ratio: 20

# Inference
num_envs: 100
num_episodes: 5
planner_num_candidates: 50
bb_num_candidates: 5
planner_ckpt: 1000000
critic_ckpt: 200000
policy_ckpt: 1000000
invdyn_ckpt: 200000
planner_use_ema: True
policy_temperature: 0.5
policy_use_ema: True

use_pretrained_dql_critic: 0
sigma_threshold: 0.1
learn_dv_value: 1
kld_target: 1
use_random_imitation: 0
dataset_type: horizon
model_type: bb # bb, bbseq
conditional_decoder: 0

# for distill
mlp_hidden_depth: 1

# hydra
hydra:
  job:
    chdir: false

